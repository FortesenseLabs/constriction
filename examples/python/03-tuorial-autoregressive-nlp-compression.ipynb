{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Compressing Natural Language With an Autoregressive Machine Learning Model and `constriction`\n",
    "\n",
    "- **Author:** Robert Bamler, University of Tuebingen\n",
    "- **Initial Publication Date:** Jan 7, 2022\n",
    "\n",
    "This is an interactive jupyter notebook.\n",
    "You can read this notebook [online](https://github.com/bamler-lab/constriction/blob/main/examples/python/03-tuorial-autoregressive-nlp-compression.ipynb) but if you want to execute any code, we recommend to [download](https://raw.githubusercontent.com/bamler-lab/constriction/main/examples/python/03-tuorial-autoregressive-nlp-compression.ipynb) it.\n",
    "\n",
    "More examples, tutorials, and reference materials are available at <https://bamler-lab.github.io/constriction/>.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook teaches you how to losslessly compress and decompress data with `constriction` using an autoregressive machine learning model.\n",
    "We will use the simple character based recurrent neural network toy-model for natural language from the [Practical PyTorch Series](https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb).\n",
    "You won't need any fancy hardware (like a GPU) to go along with this tutorial since the model is so simplistic that it can be trained on a normal consumer PC in less than 30 minutes.\n",
    "\n",
    "The compression method we develop in this tutorial is for demonstration purpose only and not meant as a proposal for practical text compression.\n",
    "While you will find that our method can compress text that is similar to the training data very well (with a 40% reduction in bitrate compared to `bzip2`), you'll also find that it will generalize poorly to other text forms and that compression and decompression are excruciatingly slow.\n",
    "The poor generalization performance is not a fundamental issue of the presented approach; it is just a result of using a very simplistic model combined with a very small training set.\n",
    "The runtime performance, too, would improve to *some* degree if we used a better suited model and if we ported the implementation to a compiled language and avoid permanent data-copying between Python, PyTorch, and `constriction`.\n",
    "However, even so, autoregressive models tend to be quite runtime-inefficient in general due to poor parallelizability.\n",
    "An alternative to autoregressive models for exploiting correlations in data compression is the so-called bits-back technique.\n",
    "An example of bits-back coding with `constriction` is provided in [this problem set](https://robamler.github.io/teaching/compress21/problem-set-05.zip) (with [solutions](https://robamler.github.io/teaching/compress21/problem-set-05-solutions.zip))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Your Environment\n",
    "\n",
    "We'll use the PyTorch deep learning framework to train and evaluate our entropy model.\n",
    "Follow the [installation instructions](https://pytorch.org/get-started/locally/) (you'll save a *lot* of download time and disk space if you install the CPU-only version).\n",
    "Then restart your jupyter kernel and test if you can import PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, make sure you have a recent enough version of `constriction` and a few other small Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: constriction~=0.2.1 in /home/robamler/.cache/pypoetry/virtualenvs/constriction-autoreg-tutorial-Qblf2bu7-py3.9/lib/python3.9/site-packages (0.2.1)\n",
      "Requirement already satisfied: tqdm~=4.62.3 in /home/robamler/.cache/pypoetry/virtualenvs/constriction-autoreg-tutorial-Qblf2bu7-py3.9/lib/python3.9/site-packages (4.62.3)\n",
      "Requirement already satisfied: unidecode~=1.3.2 in /home/robamler/.cache/pypoetry/virtualenvs/constriction-autoreg-tutorial-Qblf2bu7-py3.9/lib/python3.9/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy~=1.19 in /home/robamler/.cache/pypoetry/virtualenvs/constriction-autoreg-tutorial-Qblf2bu7-py3.9/lib/python3.9/site-packages (from constriction~=0.2.1) (1.22.0)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install constriction~=0.2.1 tqdm~=4.62.3 unidecode~=1.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart your jupyter kernel again.\n",
    "Then let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get Some Training Data\n",
    "\n",
    "We'll train our text model on the same [100,000 character Shakespeare sample](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) that was used in the [Practical PyTorch Series](https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb).\n",
    "Different from the Practical PyTorch Series, however, we'll split the data set into a training set and a test set so that we can test our compression method on text on which the model wasn't explicitly trained.\n",
    "\n",
    "Start with downloading the full data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-08 04:26:25--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1,1M) [text/plain]\n",
      "Saving to: ‘shakespeare_all.txt’\n",
      "\n",
      "shakespeare_all.txt 100%[===================>]   1,06M  3,97MB/s    in 0,3s    \n",
      "\n",
      "2022-01-08 04:26:27 (3,97 MB/s) - ‘shakespeare_all.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O shakespeare_all.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split the data set into, say, 90% training data and 10% test data by randomly assigning each line of the full data set to either one of those two subsets.\n",
    "In a real scientific project, you should usually split into more than just two sets (e.g., add an additional \"validation\" set that you then use to tune model hyperparameters).\n",
    "But we'll keep it simple for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of non-empty lines in the data set: 32777\n",
      "File `shakespeare_train.txt` has 29524 lines (90.1%).\n",
      "File `shakespeare_test.txt` has 3253 lines (9.9%).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "target_test_set_ratio = 0.1 # means that about 10% of the lines will end up in the training set.\n",
    "train_count, test_count = 0, 0\n",
    "rng = np.random.RandomState(830472) # (always set a random seed to make results reproducible)\n",
    "\n",
    "with open(\"shakespeare_all.txt\", \"r\") as in_file, \\\n",
    "     open(\"shakespeare_train.txt\", \"w\") as train_file, \\\n",
    "     open(\"shakespeare_test.txt\", \"w\") as test_file:\n",
    "    for line in in_file.readlines():\n",
    "        if line == \"\\n\" or line == \"\":\n",
    "            continue # Let's leave out empty lines\n",
    "        if rng.uniform() < target_test_set_ratio:\n",
    "            test_file.write(line)\n",
    "            test_count += 1\n",
    "        else:\n",
    "            train_file.write(line)\n",
    "            train_count += 1\n",
    "\n",
    "total_count = train_count + test_count\n",
    "print(f\"Total number of non-empty lines in the data set: {total_count}\")\n",
    "print(f\"File `shakespeare_train.txt` has {train_count} lines ({100 * train_count / total_count:.1f}%).\")\n",
    "print(f\"File `shakespeare_test.txt` has {test_count} lines ({100 * test_count / total_count:.1f}%).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train The Model\n",
    "\n",
    "We'll use the toy model described in [this tutorial](https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb).\n",
    "Luckily, someone has already extracted the relevant code blocks from the tutorial and built a command line application around it, so we'll just use that.\n",
    "\n",
    "Clone the repository into a subdirectory `char-rnn.pytorch` right next to this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/spro/char-rnn.pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of writing, the `char-rnn.pytorch` repository seems to have a small bug that will result in an exception being raised when training the model.\n",
    "But we can fix it easily.\n",
    "Open the file `char-rnn.pytorch/train.py` in a text editor, look for the line in the `train` function that reads\n",
    "\n",
    "```python\n",
    "    return loss.data[0] / args.chunk_len\n",
    "```\n",
    "\n",
    "and replace \"`[0]`\" with \"`.item()`\" so that the line now reads:\n",
    "\n",
    "```python\n",
    "    return loss.data.item() / args.chunk_len\n",
    "```\n",
    "\n",
    "If you can't find the original line then the bug has probably been fixed in the meantime.\n",
    "\n",
    "Now, start the training procedure.\n",
    "**This will take about 20-30 minutes** but you'll only have to do it once since the trained model will be saved to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000 epochs...\n",
      "  5%|██                                       | 99/2000 [01:20<18:52,  1.68it/s][1m 21s (100 5%) 1.7639]\n",
      "Whou grom to kings.\n",
      "DUKE OF ANGEL:\n",
      "Thou londers of link'd it and then himself,\n",
      "so sut you was a man, a \n",
      "\n",
      " 10%|███▉                                    | 199/2000 [02:16<19:58,  1.50it/s][2m 16s (200 10%) 1.5666]\n",
      "When wall me word of theatia black;\n",
      "My heart, thou thing so mine coruse deparet the die a game.\n",
      "Fer, l \n",
      "\n",
      " 15%|█████▉                                  | 299/2000 [03:17<21:26,  1.32it/s][3m 17s (300 15%) 1.4660]\n",
      "What have unthind that know our wild.\n",
      "Swith all o'er depring-sumserd.\n",
      "JULIET:\n",
      "Should thou say been mak \n",
      "\n",
      " 20%|███████▉                                | 399/2000 [04:15<15:58,  1.67it/s][4m 15s (400 20%) 1.4475]\n",
      "What too less me done,\n",
      "In rough him sin that ladiers, but heaven:\n",
      "Bear the death, for faces only virtu \n",
      "\n",
      " 25%|█████████▉                              | 499/2000 [05:33<15:18,  1.63it/s][5m 34s (500 25%) 1.4243]\n",
      "Whe self her favour.\n",
      "QUEEN ELIZABETH:\n",
      "We'll makes of it love to appregnant;\n",
      "And the knee but to the fa \n",
      "\n",
      " 30%|███████████▉                            | 599/2000 [06:32<12:47,  1.83it/s][6m 32s (600 30%) 1.4287]\n",
      "What by the shows and offices all.\n",
      "HENRY BOLINGBROKE:\n",
      "Go to it is the else the princed and queen,\n",
      "If I \n",
      "\n",
      " 35%|█████████████▉                          | 699/2000 [07:51<29:08,  1.34s/it][7m 52s (700 35%) 1.4206]\n",
      "What stalty bear that eyes are has none?\n",
      "Hold some lords,\n",
      "And absting such as you I comes, that hath b \n",
      "\n",
      " 40%|███████████████▉                        | 799/2000 [09:54<20:36,  1.03s/it][9m 55s (800 40%) 1.3998]\n",
      "What the good men'd nine lase here.\n",
      "HENRY BOLINGBROKE:\n",
      "How now: and orature of Irelain?\n",
      "You with he ha \n",
      "\n",
      " 45%|█████████████████▉                      | 899/2000 [11:51<34:50,  1.90s/it][11m 52s (900 45%) 1.3719]\n",
      "Who stay, my lord, and Stand of Exeters!\n",
      "The words more flutchers odd-denier for thy little at\n",
      "We That \n",
      "\n",
      " 50%|███████████████████▉                    | 999/2000 [13:36<20:46,  1.25s/it][13m 39s (1000 50%) 1.3735]\n",
      "Whan, therefore shall speech,\n",
      "Which to show blood Clifford, have your villain.\n",
      "Here's save sweet to us \n",
      "\n",
      " 55%|█████████████████████▍                 | 1099/2000 [15:37<16:38,  1.11s/it][15m 38s (1100 55%) 1.3787]\n",
      "Whance?\n",
      "BUCKINGHAM:\n",
      "So part say us twave and other like a broke her.\n",
      "We must a man of her old with the \n",
      "\n",
      " 60%|███████████████████████▍               | 1199/2000 [17:22<13:18,  1.00it/s][17m 23s (1200 60%) 1.3604]\n",
      "Who be as they that an old mistrice,\n",
      "The restants I to your truth, that how he say:\n",
      "For here's in that \n",
      "\n",
      " 65%|█████████████████████████▎             | 1299/2000 [19:17<13:25,  1.15s/it][19m 19s (1300 65%) 1.3868]\n",
      "What thee beat deserves 'mong Marcius:\n",
      "Perform'd, thou art thou is my state with cling,\n",
      "That he was th \n",
      "\n",
      " 70%|███████████████████████████▎           | 1399/2000 [21:20<12:09,  1.21s/it][21m 21s (1400 70%) 1.3862]\n",
      "Whanks, what is no cleads,\n",
      "I was for a wed of mistress were in little day,\n",
      "As he made me the leave me  \n",
      "\n",
      " 75%|█████████████████████████████▏         | 1499/2000 [23:06<04:47,  1.74it/s][23m 7s (1500 75%) 1.3641]\n",
      "What I will be her the friar, cozal:\n",
      "The years should be sets\n",
      "And which 'there this dear eyes, Norther \n",
      "\n",
      " 80%|███████████████████████████████▏       | 1599/2000 [24:12<05:29,  1.22it/s][24m 13s (1600 80%) 1.3621]\n",
      "When thou art past is the liest through them\n",
      "Here of his supping what are the Spant;\n",
      "And thou wert for \n",
      "\n",
      " 85%|█████████████████████████████████▏     | 1699/2000 [25:16<03:11,  1.57it/s][25m 17s (1700 85%) 1.3626]\n",
      "Why so, come it in Padue, thinks.\n",
      "Even country of men, give me upon the burth;\n",
      "Stark of the Lords, I w \n",
      "\n",
      " 90%|███████████████████████████████████    | 1799/2000 [26:18<02:13,  1.50it/s][26m 19s (1800 90%) 1.3710]\n",
      "Why, Cloingly seem to the time and so mides.\n",
      "LEONTES:\n",
      "MIRANDA:\n",
      "That thou goes:\n",
      "The eneman of us he can \n",
      "\n",
      " 95%|█████████████████████████████████████  | 1899/2000 [27:19<00:58,  1.71it/s][27m 20s (1900 95%) 1.3491]\n",
      "Why we will thy good a short,\n",
      "And the king with his head as earl meat the houseless\n",
      "Not be a princely  \n",
      "\n",
      "100%|██████████████████████████████████████▉| 1999/2000 [28:38<00:00,  1.72it/s][28m 39s (2000 100%) 1.3499]\n",
      "What hath enough, whiles are haste. Which have with us.\n",
      "May not stand he lions, and that far reverence \n",
      "\n",
      "100%|███████████████████████████████████████| 2000/2000 [28:39<00:00,  1.16it/s]\n",
      "Saving...\n",
      "Saved as shakespeare_train.pt\n"
     ]
    }
   ],
   "source": [
    "!python char-rnn.pytorch/train.py shakespeare_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above training script should print a few lines of text after each completed 5% of training progress.\n",
    "The generated text snippets are random samples from the trained model.\n",
    "You should be able to observe that the quality of the sampled text should improve as training proceeds.\n",
    "\n",
    "At the end of the training cycle, the generated text will not be perfect, but that's OK for the purpose of compression.\n",
    "When we'll use the trained model to compress some new text below, we won't be blindly following the model's predictions as in the randomly generated text here.\n",
    "Instead, we'll compare the model's predictions with the actual text that we want to compress, and we'll then essentially encode the difference.\n",
    "Thus, model predictions don't need to be perfect; as long as they're better than a completely random (uniform) guess, they will allow us to reduce the bitrate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: While the Model is Being Trained ...\n",
    "\n",
    "While you wait for the model to be trained, let's start thinking about how we'll use the trained model for our compression method.\n",
    "\n",
    "### Overall Encoder/Decoder Design\n",
    "\n",
    "The model is an autoregressive model that processes one character after the other.\n",
    "Each step takes the previous character as input, updates some internal hidden state, and outputs a probability distribution over all possible values for the next character.\n",
    "\n",
    "This autoregressive model architecture pretty much dictates how our compression method must operate:\n",
    "\n",
    "- The *encoder* reads the message that we want to compress character by character and updates the autoregressive model with each step.\n",
    "  It uses the output of the model at each step to define an entropy model for encoding the next character.\n",
    "- The *decoder* decodes one character at a time and uses it to perform the exact same model updates as the encoder did.\n",
    "  The entropy model used for each decoding step is defined by the model output from the previous step.\n",
    "\n",
    "The very first model update on both encoder and decoder side is a bit subtle because we don't yet have a \"previous\" character to provide as input for the model update here.\n",
    "We'll just make up a fake character that we pretend exists before the start of the actual message, and we'll inject this fake character at the very beginning to both the encoder and the decoder.\n",
    "Let's use the newline character `\"\\n\"` for this purpose since this seems like a character that could indeed naturally precede the beginning of any message.\n",
    "\n",
    "Since encoding and decoding iterate over the characters in the same order, we'll need to use an entropy coder with *queue* semantics (i.e., \"first in first out\").\n",
    "The `constriction` library provides two entropy coders with queue semantics: [Range Coding](https://bamler-lab.github.io/constriction/apidoc/python/stream/queue.html) and [Huffman Coding](https://bamler-lab.github.io/constriction/apidoc/python/symbol.html).\n",
    "We'll use Range Coding here since it has better compression performance.\n",
    "At the end of this notebook you'll find an empirical comparison to Huffman Coding (which will turn out to perform worse).\n",
    "\n",
    "### How to Implement Iterative Model Updates\n",
    "\n",
    "So how are we going to perform these iterative model updates described above in practice?\n",
    "Let's just start from how it's done in the the implementation of the model (which you cloned into the subdirectory `char-rnn.pytorch` in Step 3 above) and adapt it to our needs.\n",
    "The file `generate.py` seems like a good place to look for inspiration since generating random text from the model is not all that different from what we're trying to do: both have to update the model character by character, continuously obtaining a probability distribution over the next character.\n",
    "The only difference is that `generate.py` uses the obtained probability distribution to sample from it while we'll use it to define an entropy model.\n",
    "\n",
    "The function `generate` in `char-rnn.pytorch/generate.py` shows us how to do all the steps we need.\n",
    "The function takes a model as argument, which is—confusingly—called `decoder`.\n",
    "The function body starts by initializing a hidden state as follows,\n",
    "\n",
    "```python\n",
    "    hidden = decoder.init_hidden(1)\n",
    "```\n",
    "\n",
    "After some more initializations, the function enters a loop that iteratively updates the model as follows:\n",
    "\n",
    "```python\n",
    "    output, hidden = decoder(inp, hidden)\n",
    "```\n",
    "\n",
    "where `inp` is the \"input\", i.e., the previous character, represented (for technical reasons) as an integer PyTorch tensor of length `1`.\n",
    "The above model update returns `output` and the updated `hidden` state.\n",
    "On the next line of code, `output` gets scaled by some temperature and exponentiated before it is interpreted as an (unnormalized) probability distribution by being passed into `torch.multinomial`.\n",
    "Thus, `output` seems to be a tensor of logits, defining the probability distribution for the next character.\n",
    "Finally, after drawing a sample `top_i` from the `torch.multinomial` distribution, the function maps the sampled integer to a character as follows:\n",
    "\n",
    "```python\n",
    "    predicted_char = all_characters[top_i]\n",
    "```\n",
    "\n",
    "Thus, there seems to be some fixed string `all_characters` that has the character with integer representation `i` at its `i`'th position.\n",
    "Let's bring this string into scope (and while we're at it, let's also import some other stuff we'll need below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import constriction\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add subdirectory `char-rnn.pytorch` to Python's path so we can import some stuff from it.\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"char-rnn.pytorch\"))\n",
    "from model import *\n",
    "from helpers import read_file, all_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b\f\n"
     ]
    }
   ],
   "source": [
    "print(all_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement The Encoder\n",
    "\n",
    "We now have everything we need to know to implement the encoder and decoder.\n",
    "Let's start with the encoder, and define a function `compress_file` that takes the path to a file, compresses it, and writes the output to a different file.\n",
    "Since our compression method will turn out to be very slow (see introduction), we'll also introduce an optional argument `max_chars` that will allow the caller to compress only the first `max_chars` characters from the input file and stop after that.\n",
    "The function `compress_file` will also expect a `model` argument where the caller will have to pass in the trained model (this was called `decoder` in the file `generate.py` discussed above, but we'll call it `model` here to avoid confusion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_file(model, in_filename, out_filename, max_chars=None):\n",
    "    message, _ = read_file(in_filename) # (`read_file` defined in `char-rnn.pytorch/helpers.py`)\n",
    "    if max_chars is not None:\n",
    "        message = message[:max_chars]   # Truncate message to at most `max_chars` characters.\n",
    "\n",
    "    # Initialize the hidden state and model input as discussed above:\n",
    "    hidden = model.init_hidden(1) # (same as in `generate.py` discussed above)\n",
    "    input_char = torch.tensor([all_characters.index('\\n')], dtype=torch.int64) # \"fake\" character that we pretend precedes the message\n",
    "\n",
    "    # Instantiate an empty Range Coder onto which we'll accumulate compressed data:\n",
    "    encoder = constriction.stream.queue.RangeEncoder()\n",
    "\n",
    "    # Iterate over the message and encode it character by character, updating the model as we go:\n",
    "    for char in tqdm(message):\n",
    "        output, hidden = model(input_char, hidden) # update the model (as in `generate.py`)\n",
    "\n",
    "        # Turn the `output` into an entropy model and encode the character with it:\n",
    "        logits = output.data.view(-1)\n",
    "        logits = logits - logits.max() # \"Log-Sum-Exp trick\" for numerical stability\n",
    "        unnormalized_probs = logits.exp().numpy().astype(np.float64)\n",
    "        entropy_model = constriction.stream.model.Categorical(unnormalized_probs)\n",
    "        char_index = all_characters.index(char)\n",
    "        encoder.encode(char_index, entropy_model)\n",
    "\n",
    "        # Prepare for next model update:\n",
    "        input_char[0] = char_index\n",
    "\n",
    "    # Save the compressed data to a file\n",
    "    print(f\"Compressed {len(message)} characters into {encoder.num_bits()} bits ({encoder.num_bits() / len(message):.2f} bits per character).\")\n",
    "    compressed = encoder.get_compressed()\n",
    "    if sys.byteorder != \"little\":\n",
    "        # Let's always save data in the same byte order so compressed files can be transferred across computer architectures.\n",
    "        compressed.byteswap(inplace=True)\n",
    "    compressed.tofile(out_filename)\n",
    "    print(f'Wrote compressed data to file \"{out_filename}\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main part that distinguishes our function `compress_file` from the function `generate` in `generate.py` discussed above is that, after each model update, we use the model to encode an (already given) character rather than to sample a character.\n",
    "There are two slightly subtle step here:\n",
    "first, we subtract the constant `logits.max()` from all elements of `logits` before exponentiating them.\n",
    "Such a global shift in logit-space has no effect (apart from rounding errors) on the resulting probability distribution since it will correspond to a global scaling factor after exponentiation.\n",
    "We perform this operation out of an abundance of caution to prevent numerical overflow when we exponentiate `logits` on the next line.\n",
    "Second, we construct the `Categorical` entropy model from a tensor of *unnormalized* probabilities.\n",
    "That's OK according to [the documentation](https://bamler-lab.github.io/constriction/apidoc/python/stream/model.html#constriction.stream.model.Categorical), `constriction` will have to make sure the distribution is exactly normalize in fixed-point arithmetic anyway.\n",
    "\n",
    "### Implement The Decoder\n",
    "\n",
    "Let's also implement a function `decompress_file` that recovers the message from its compressed representation so that we can prove that the encoder did not discard any information.\n",
    "The decoder operates very similar to the encoder, except that it starts by loading compressed data from a file instead of the message, and it initializes a `RangeDecoder` from it, from which it then decodes one symbol at a time in the iteration.\n",
    "One important difference to the encoder is that, with our current autoregressive model, the decoder cannot detect the end of the message.\n",
    "Therefore, we have to provide the message length (`num_chars`) as an argument to the decoder function.\n",
    "In a real deployment, you'll probably want to either transmit the message length as an explicit initial symbol, or you could add an \"End of File\" sentinel symbol to the alphabet (`all_characters`) and append this symbol to the message on the encoder side to signal to the decoder that it should stop processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_file(model, in_filename, out_filename, num_chars):\n",
    "    # Load the compressed data into a `RangeDecoder`:`\n",
    "    compressed = np.fromfile(in_filename, dtype=np.uint32)\n",
    "    if sys.byteorder != \"little\":\n",
    "        compressed.byteswap(inplace=True) # restores native byte order (\"endianness\").\n",
    "    print(f\"Loaded {32 * len(compressed)} bits of compressed data.\")\n",
    "    decoder = constriction.stream.queue.RangeDecoder(compressed)\n",
    "\n",
    "    # Initialize the hidden state and model input exactly as in the encoder:\n",
    "    hidden = model.init_hidden(1) # (same as in `generate.py` discussed above)\n",
    "    input_char = torch.tensor([all_characters.index('\\n')], dtype=torch.int64) # \"fake\" character that we pretend precedes the message\n",
    "\n",
    "    # Decode the message character by character, updating the model as we go:\n",
    "    with open(out_filename, \"w\") as out_file:\n",
    "        for _ in tqdm(range(num_chars)):\n",
    "            # Update model and optain (unnormalized) probabilities, exactly as in the encoder:\n",
    "            output, hidden = model(input_char, hidden)\n",
    "            logits = output.data.view(-1)\n",
    "            logits = logits - logits.max()\n",
    "            unnormalized_probs = logits.exp().numpy().astype(np.float64)\n",
    "            entropy_model = constriction.stream.model.Categorical(unnormalized_probs)\n",
    "            \n",
    "            # This time, use the `entropy_model` for *decoding* to obtain the next character:\n",
    "            char_index = decoder.decode(entropy_model)\n",
    "            char = all_characters[char_index]\n",
    "            out_file.write(char)\n",
    "\n",
    "            # Prepare for next model update, exactly as in the encoder:\n",
    "            input_char[0] = char_index\n",
    "\n",
    "    print(f'Wrote decompressed data to file \"{out_filename}\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Try It Out\n",
    "\n",
    "If you've followed along and taken the time to understand the encoder/decoder implementation in Step 4 above then the model should have finished training by now.\n",
    "Load it into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"shakespeare_train.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try out if our implementation can indeed compress and decompress a text file with this model.\n",
    "We'll compress the *test* subset of our data set so that we test our method on text that was not used for training (albeit, admittedly, the test data is very similar to the training data since both were written by the same author):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1013.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed 10000 characters into 20704 bits (2.07 bits per character).\n",
      "Wrote compressed data to file \"shakespeare_test.txt.compressed\".\n"
     ]
    }
   ],
   "source": [
    "compress_file(model, \"shakespeare_test.txt\", \"shakespeare_test.txt.compressed\", max_chars=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you didn't change anything in the training schedule then you should get a bitrate of about 2.1 bits per character.\n",
    "Before we compare this bitrate to that of general-purpose compression methods, let's first verify that the compression method is actually correct.\n",
    "Decode the compressed file again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20704 bits of compressed data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1098.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote decompressed data to file \"shakespeare_test.txt.decompressed\".\n"
     ]
    }
   ],
   "source": [
    "decompress_file(model, \"shakespeare_test.txt.compressed\", \"shakespeare_test.txt.decompressed\", 10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick peak in the original and reconstructed text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> shakespeare_test.txt <==\n",
      "Before we proceed any further, hear me speak.\n",
      "All:\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "would yield us but the superfluity, while it were\n",
      "but they think we are too dear: the leanness that\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "report fort, but that he pays himself with being proud.\n",
      "First Citizen:\n",
      "Come, come.\n",
      "\n",
      "==> shakespeare_test.txt.decompressed <==\n",
      "Before we proceed any further, hear me speak.\n",
      "All:\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "would yield us but the superfluity, while it were\n",
      "but they think we are too dear: the leanness that\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "report fort, but that he pays himself with being proud.\n",
      "First Citizen:\n",
      "Come, come.\n"
     ]
    }
   ],
   "source": [
    "!head shakespeare_test.txt shakespeare_test.txt.decompressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beginnings certainly look similar.\n",
    "But let's check more thoroughly.\n",
    "Remember that we only encoded and decoded the first 10,000 characters of the test data, so that's what we have to compare to (note: the test turns out to be pure ASCII, so the first 10,000 characters map exactly to the first 10,000 bytes):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -c 10000 shakespeare_test.txt | diff - shakespeare_test.txt.decompressed  # If this prints no output we're good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluation\n",
    "\n",
    "There's a lot we could analyze now:\n",
    "\n",
    "- How do the bitrates of our method compare to general-purpose compression methods like `gzip`, `bzip2`, and `xz`?\n",
    "- How well does our method generalize to other text, ranging from other English text by a different author all the way to text in a different language?\n",
    "- Where do the encoder and decoder spend most of their runtime?\n",
    "\n",
    "We'll just address the first question here and leave the others to the reader.\n",
    "Let's compress the same first 10,000 characters of the test data with `gzip`, `bzip2`, and `xz` (if installed on your system):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -c 10000 shakespeare_test.txt | gzip --best > shakespeare_test.txt.gzip\n",
    "!head -c 10000 shakespeare_test.txt | bzip2 --best > shakespeare_test.txt.bz2\n",
    "!head -c 10000 shakespeare_test.txt | xz --best > shakespeare_test.txt.xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's compare the sizes of the compressed files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 robamler robamler  4302 Jan  7 21:37 shakespeare_test.txt.bz2\n",
      "-rw-rw-r-- 1 robamler robamler  2588 Jan  7 21:37 shakespeare_test.txt.compressed\n",
      "-rw-rw-r-- 1 robamler robamler 10000 Jan  7 21:37 shakespeare_test.txt.decompressed\n",
      "-rw-rw-r-- 1 robamler robamler  4814 Jan  7 21:37 shakespeare_test.txt.gzip\n",
      "-rw-rw-r-- 1 robamler robamler  4788 Jan  7 21:37 shakespeare_test.txt.xz\n"
     ]
    }
   ],
   "source": [
    "!ls -l shakespeare_test.txt.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite using a very simple model that we took from a tutorial completely unrelated to data compression, our compression method reduces the bitrate compared to `bzip2` by 40%.\n",
    "Of course, we shouldn't read too much into this since we trained the model on data that is very similar to the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Huffman Coding\n",
    "\n",
    "Our above compression method uses Range Coding for the entropy coder.\n",
    "The `constriction` library also provides another entropy coder with \"queue\" semantics: Huffman coding.\n",
    "The API for Huffman coding is somewhat different to that of Range Coding because of the very different nature of the two algorithms, but it's easy to port our encoder and decoder to Huffman coding:\n",
    "\n",
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_file_huffman(model, in_filename, out_filename, max_chars=None):\n",
    "    message, _ = read_file(in_filename) # (`read_file` defined in `char-rnn.pytorch/helpers.py`)\n",
    "    if max_chars is not None:\n",
    "        message = message[:max_chars]   # Truncate message to at most `max_chars` characters.\n",
    "\n",
    "    # Initialize the hidden state and model input as discussed above:\n",
    "    hidden = model.init_hidden(1) # (same as in `generate.py` discussed above)\n",
    "    input_char = torch.tensor([all_characters.index('\\n')], dtype=torch.int64) # \"fake\" character that we pretend precedes the message\n",
    "\n",
    "    # Instantiate an empty `QueueEncoder` onto which we'll accumulate compressed data:\n",
    "    encoder = constriction.symbol.QueueEncoder()                                       # <-- CHANGED LINE\n",
    "\n",
    "    # Iterate over the message and encode it character by character, updating the model as we go:\n",
    "    for char in tqdm(message):\n",
    "        output, hidden = model(input_char, hidden) # update the model (as in `generate.py`)\n",
    "\n",
    "        # Turn the `output` into an entropy model and encode the character with it:\n",
    "        logits = output.data.view(-1)\n",
    "        logits = logits - logits.max() # \"Log-Sum-Exp trick\" for numerical stability\n",
    "        unnormalized_probs = logits.exp().numpy().astype(np.float64)\n",
    "        codebook = constriction.symbol.huffman.EncoderHuffmanTree(unnormalized_probs)  # <-- CHANGED LINE\n",
    "        char_index = all_characters.index(char)\n",
    "        encoder.encode_symbol(char_index, codebook)                                    # <-- CHANGED LINE\n",
    "\n",
    "        # Prepare for next model update:\n",
    "        input_char[0] = char_index\n",
    "\n",
    "    # Save the compressed data to a file\n",
    "    compressed, num_bits = encoder.get_compressed()                                    # <-- CHANGED LINE\n",
    "    print(f\"Compressed {len(message)} characters into {num_bits} bits ({num_bits / len(message):.2f} bits per character).\")\n",
    "    if sys.byteorder != \"little\":\n",
    "        # Let's always save data in the same byte order so compressed files can be transferred across computer architectures.\n",
    "        compressed.byteswap(inplace=True)\n",
    "    compressed.tofile(out_filename)\n",
    "    print(f'Wrote compressed data to file \"{out_filename}\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_file_huffman(model, in_filename, out_filename, num_chars):\n",
    "    # Load the compressed data into a `RangeDecoder`:`\n",
    "    compressed = np.fromfile(in_filename, dtype=np.uint32)\n",
    "    if sys.byteorder != \"little\":\n",
    "        compressed.byteswap(inplace=True) # restores native byte order (\"endianness\").\n",
    "    print(f\"Loaded {32 * len(compressed)} bits of compressed data.\")\n",
    "    decoder = constriction.symbol.QueueDecoder(compressed)                                 # <-- CHANGED LINE\n",
    "\n",
    "    # Initialize the hidden state and model input exactly as in the encoder:\n",
    "    hidden = model.init_hidden(1) # (same as in `generate.py` discussed above)\n",
    "    input_char = torch.tensor([all_characters.index('\\n')], dtype=torch.int64) # \"fake\" character that we pretend precedes the message\n",
    "\n",
    "    # Decode the message character by character, updating the model as we go:\n",
    "    with open(out_filename, \"w\") as out_file:\n",
    "        for _ in tqdm(range(num_chars)):\n",
    "            # Update model and optain (unnormalized) probabilities, exactly as in the encoder:\n",
    "            output, hidden = model(input_char, hidden)\n",
    "            logits = output.data.view(-1)\n",
    "            logits = logits - logits.max()\n",
    "            unnormalized_probs = logits.exp().numpy().astype(np.float64)\n",
    "            codebook = constriction.symbol.huffman.DecoderHuffmanTree(unnormalized_probs)  # <-- CHANGED LINE\n",
    "            \n",
    "            # This time, use the `codebook` for *decoding* to obtain the next character:\n",
    "            char_index = decoder.decode_symbol(codebook)                                   # <-- CHANGED LINE\n",
    "            char = all_characters[char_index]\n",
    "            out_file.write(char)\n",
    "\n",
    "            # Prepare for next model update, exactly as in the encoder:\n",
    "            input_char[0] = char_index\n",
    "\n",
    "    print(f'Wrote decompressed data to file \"{out_filename}\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it Out Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1097.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed 10000 characters into 23336 bits (2.33 bits per character).\n",
      "Wrote compressed data to file \"shakespeare_test.txt.compressed-huffman\".\n"
     ]
    }
   ],
   "source": [
    "compress_file_huffman(model, \"shakespeare_test.txt\", \"shakespeare_test.txt.compressed-huffman\", max_chars=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison: when we used a Range Coder we got a better bitrate of only about 2.1 bits per character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 23360 bits of compressed data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:14<00:00, 693.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote decompressed data to file \"shakespeare_test.txt.decompressed-huffman\".\n"
     ]
    }
   ],
   "source": [
    "decompress_file_huffman(model, \"shakespeare_test.txt.compressed-huffman\", \"shakespeare_test.txt.decompressed-huffman\", 10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify correctness again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -c 10000 shakespeare_test.txt | diff - shakespeare_test.txt.decompressed-huffman  # If this prints no output we're good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've discussed how you can use `constriction`'s entropy coders with an entropy model that is an autoregressive machine-learning model.\n",
    "Autoregressive models allow you to model correlations between symbols, and exploit them to improve compression performance.\n",
    "An alternative method for exploiting correlations in data compression is the so-called bits-back technique, which applies to latent variable models that tend to be better parallelizable than autoregressive models.\n",
    "An example of bits-back coding with `constriction` is provided in [this problem set](https://robamler.github.io/teaching/compress21/problem-set-05.zip) (with [solutions](https://robamler.github.io/teaching/compress21/problem-set-05-solutions.zip))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
